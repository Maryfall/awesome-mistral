# Awesome Mistral [![Awesome](https://awesome.re/badge.svg)](https://awesome.re) ![Last Updated](https://img.shields.io/github/last-commit/samouraiworld/awesome-mistral)

> A curated list of awesome resources, tools, libraries, and projects for the Mistral AI ecosystem.

Mistral AI is a Paris-based AI company building open-weight, high-performance large language models. Founded in 2023, Mistral has quickly become a leading force in open-source AI, offering models that rival proprietary alternatives while remaining accessible to developers worldwide.

This repository maps and curates the entire Mistral.ai ecosystem for AI engineers, researchers, startup founders, and open-source contributors.

**Legend:**
- ğŸ§  Official Mistral AI
- ğŸŒ Community project
- ğŸ§ª Experimental

---

## Contents

- [Why Mistral?](#why-mistral)
- [Official Mistral Resources](#official-mistral-resources)
- [Models](#models)
- [Community Fine-Tuned Models](#community-fine-tuned-models)
- [SDKs & APIs](#sdks--apis)
- [Inference & Deployment](#inference--deployment)
- [Fine-Tuning & Training](#fine-tuning--training)
- [Model Merging & Quantization](#model-merging--quantization)
- [Agents & Orchestration](#agents--orchestration)
- [Tooling & Dev Experience](#tooling--dev-experience)
- [Community Projects](#community-projects)
- [Demos & Examples](#demos--examples)
- [Tutorials & Guides](#tutorials--guides)
- [Benchmarks & Evaluation](#benchmarks--evaluation)
- [Research & Papers](#research--papers)
- [Talks & Media](#talks--media)
- [Ecosystem & Community](#ecosystem--community)
- [Contributing](#contributing)
- [License](#license)

---

## Why Mistral?

Mistral AI offers a compelling alternative in the LLM landscape:

| Aspect | Mistral Advantage |
|--------|-------------------|
| **Open Weights** | Models like Mistral 7B and Mixtral are fully open-weight, enabling local deployment, fine-tuning, and full control |
| **Efficiency** | Mistral 7B outperforms Llama 2 13B; Mixtral 8x7B matches GPT-3.5 with only 12.9B active parameters |
| **European Sovereignty** | Paris-based company offering GDPR-compliant, EU-hosted API options |
| **Cost Efficiency** | Competitive API pricing; open models enable free self-hosting |
| **Innovation** | Pioneered efficient MoE architectures and sliding window attention in open models |

---

## Official Mistral Resources

- ğŸ§  [Mistral AI](https://mistral.ai) â€“ Official company website with product information and announcements.
- ğŸ§  [Mistral AI Documentation](https://docs.mistral.ai) â€“ Comprehensive API documentation, guides, and model specifications.
- ğŸ§  [Mistral AI Console](https://console.mistral.ai) â€“ Web interface for API key management and model access.
- ğŸ§  [Mistral AI GitHub](https://github.com/mistralai) â€“ Official GitHub organization with 22+ repositories.
- ğŸ§  [mistral-inference](https://github.com/mistralai/mistral-inference) â­ 10k+ â€“ Official inference library for running Mistral models.
- ğŸ§  [mistral-finetune](https://github.com/mistralai/mistral-finetune) â­ 3k+ â€“ Official lightweight LoRA-based fine-tuning library.
- ğŸ§  [Mistral Cookbook](https://github.com/mistralai/cookbook) â­ 2k+ â€“ Official notebooks and examples for common use cases.
- ğŸ§  [mistral-common](https://github.com/mistralai/mistral-common) â€“ Official tokenization and pre-processing library.
- ğŸ§  [Platform Docs Public](https://github.com/mistralai/platform-docs-public) â€“ Open-source documentation repository.

---

## Models

### Flagship Models (API)

| Model | Parameters | Context | Best For |
|-------|------------|---------|----------|
| **Mistral Large** | 123B | 128k | Complex reasoning, multilingual, code generation |
| **Mistral Medium** | â€” | 32k | Balanced performance-to-cost ratio |
| **Mistral Small** | 24B | 128k | Low-latency, cost-sensitive applications |

### Open-Weight Models

- ğŸ§  [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1) â€“ Compact 7B model outperforming Llama 2 13B on most benchmarks.
- ğŸ§  [Mistral 7B Instruct v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3) â€“ Latest instruction-tuned variant with function calling.
- ğŸ§  [Mixtral 8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1) â€“ Sparse MoE with 46.7B total / 12.9B active parameters.
- ğŸ§  [Mixtral 8x7B Instruct](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) â€“ Instruction-tuned MoE variant.
- ğŸ§  [Mixtral 8x22B](https://huggingface.co/mistral-community/Mixtral-8x22B-v0.1) â€“ Large-scale MoE with 141B total / 39B active parameters.

### Specialized Models

- ğŸ§  **Codestral** â€“ Code-specialized model for 80+ programming languages.
- ğŸ§  **Devstral** â€“ Developer-focused model for coding assistance and software development.
- ğŸ§  **Pixtral** â€“ Multimodal model with vision capabilities.
- ğŸ§  **Mathstral** â€“ Mathematics-specialized for reasoning and problem-solving.

---

## Community Fine-Tuned Models

High-quality community fine-tunes built on Mistral base models:

### Instruction & Chat

- ğŸŒ [OpenHermes-2.5-Mistral-7B](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B) â€“ GPT-4 quality instruction-tuned by Teknium.
- ğŸŒ [Zephyr-7B-beta](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta) â€“ DPO-trained by HuggingFace H4, outperforms 70B on MT-Bench.
- ğŸŒ [Nous-Hermes-2-Mistral-7B-DPO](https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO) â€“ DPO-enhanced with strong benchmark scores.
- ğŸŒ [Hermes-2-Pro-Mistral-7B](https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B) â€“ Function calling and JSON mode specialist.
- ğŸŒ [OpenChat-3.5-0106](https://huggingface.co/openchat/openchat-3.5-0106) â€“ C-RLFT trained, ChatGPT-comparable performance.
- ğŸŒ [Dolphin-2.8-Mistral-7B](https://huggingface.co/cognitivecomputations/dolphin-2.8-mistral-7b-v02) â€“ Uncensored model by Eric Hartford.

### Specialized

- ğŸŒ [MistralLite](https://huggingface.co/amazon/MistralLite) â€“ AWS-optimized with 32k context window.
- ğŸŒ [Mistral-7B-OpenOrca](https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca) â€“ Trained on OpenOrca dataset.
- ğŸŒ [WizardMath-7B-V1.1](https://huggingface.co/WizardLM/WizardMath-7B-V1.1) â€“ Math-specialized Mistral fine-tune.

### Quantized Model Collections

- ğŸŒ [TheBloke](https://huggingface.co/TheBloke) â€“ Extensive GGUF/AWQ/GPTQ quantized model repository.
- ğŸŒ [bartowski](https://huggingface.co/bartowski) â€“ High-quality GGUF quantizations.

---

## SDKs & APIs

### Official SDKs

- ğŸ§  [client-python](https://github.com/mistralai/client-python) â€“ Official Python client library.
- ğŸ§  [@mistralai/mistralai](https://www.npmjs.com/package/@mistralai/mistralai) â€“ Official TypeScript/JavaScript SDK.

### Community SDKs

- ğŸŒ [mistral.rs](https://github.com/EricLBuehler/mistral.rs) â€“ Blazingly fast Rust inference with ISQ, LoRA, quantization.
- ğŸŒ [mistral-go](https://github.com/Gage-Technologies/mistral-go) â€“ Go client for Mistral AI API.
- ğŸŒ [@ai-sdk/mistral](https://www.npmjs.com/package/@ai-sdk/mistral) â€“ Vercel AI SDK provider.
- ğŸŒ [@langchain/mistralai](https://www.npmjs.com/package/@langchain/mistralai) â€“ LangChain.js integration.

### Official Libraries

- ğŸ§  [mistral-common](https://github.com/mistralai/mistral-common) â€“ Tokenization and pre-processing.
- ğŸ§  [mistral-vibe](https://github.com/mistralai/mistral-vibe) â­ 2.5k+ â€“ Minimal CLI coding agent.

---

## Inference & Deployment

### High-Performance Inference

- ğŸŒ [vLLM](https://github.com/vllm-project/vllm) â­ 35k+ â€“ High-throughput with PagedAttention. Excellent Mistral support.
- ğŸŒ [Text Generation Inference](https://github.com/huggingface/text-generation-inference) â€“ Hugging Face's production inference server.
- ğŸŒ [llama.cpp](https://github.com/ggerganov/llama.cpp) â­ 70k+ â€“ CPU/GPU inference with GGUF quantization.
- ğŸŒ [ExLlamaV2](https://github.com/turboderp/exllamav2) â€“ Fast inference with EXL2 quantization.
- ğŸŒ [SGLang](https://github.com/sgl-project/sglang) â€“ Fast serving with RadixAttention.

### Local Inference

- ğŸŒ [Ollama](https://ollama.com) â­ 100k+ â€“ Simple CLI for local Mistral models.
- ğŸŒ [LM Studio](https://lmstudio.ai) â€“ Desktop GUI for local LLMs.
- ğŸŒ [Jan](https://jan.ai) â€“ Open-source ChatGPT alternative running locally.
- ğŸŒ [GPT4All](https://gpt4all.io) â€“ Local inference with Mistral support.
- ğŸŒ [Msty](https://msty.app) â€“ Desktop app for running local LLMs.

### Cloud & Container Deployment

- ğŸŒ [LocalAI](https://github.com/mudler/LocalAI) â­ 25k+ â€“ OpenAI-compatible local API server.
- ğŸŒ [SkyPilot](https://github.com/skypilot-org/skypilot) â€“ Run on any cloud with cost optimization.
- ğŸ§ª [MLC LLM](https://github.com/mlc-ai/mlc-llm) â€“ Universal deployment across hardware backends.

---

## Fine-Tuning & Training

### Fine-Tuning Frameworks

- ğŸ§  [mistral-finetune](https://github.com/mistralai/mistral-finetune) â€“ Official LoRA fine-tuning library.
- ğŸŒ [Axolotl](https://github.com/axolotl-ai-cloud/axolotl) â€“ Streamlined LoRA/QLoRA/full fine-tuning.
- ğŸŒ [Unsloth](https://github.com/unslothai/unsloth) â­ 20k+ â€“ 2-5x faster fine-tuning, 80% less memory.
- ğŸŒ [Hugging Face PEFT](https://github.com/huggingface/peft) â€“ Parameter-Efficient Fine-Tuning.
- ğŸŒ [Hugging Face TRL](https://github.com/huggingface/trl) â€“ RLHF and DPO training.
- ğŸŒ [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) â­ 35k+ â€“ Unified fine-tuning framework.
- ğŸŒ [torchtune](https://github.com/pytorch/torchtune) â€“ PyTorch-native fine-tuning.

### Training Infrastructure

- ğŸŒ [DeepSpeed](https://github.com/microsoft/DeepSpeed) â€“ Distributed training optimization.
- ğŸŒ [Hugging Face Accelerate](https://github.com/huggingface/accelerate) â€“ Simple distributed training.

---

## Model Merging & Quantization

### Model Merging

- ğŸŒ [MergeKit](https://github.com/arcee-ai/mergekit) â­ 5k+ â€“ Toolkit for merging LLMs (SLERP, TIES, DARE).
- ğŸŒ [LazyMergeKit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb) â€“ Colab notebook for easy merging.

### Quantization Tools

- ğŸŒ [llama.cpp](https://github.com/ggerganov/llama.cpp) â€“ GGUF quantization (Q4, Q5, Q8).
- ğŸŒ [AutoGPTQ](https://github.com/AutoGPTQ/AutoGPTQ) â€“ GPTQ quantization.
- ğŸŒ [AutoAWQ](https://github.com/casper-hansen/AutoAWQ) â€“ AWQ quantization.
- ğŸŒ [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) â€“ 4-bit and 8-bit quantization.
- ğŸŒ [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) â€“ Quantization format specification.

---

## Agents & Orchestration

### Agent Frameworks

- ğŸŒ [LangChain](https://github.com/langchain-ai/langchain) â­ 95k+ â€“ LLM app framework with native Mistral support.
- ğŸŒ [LlamaIndex](https://github.com/run-llama/llama_index) â­ 37k+ â€“ Data framework for RAG with Mistral.
- ğŸŒ [CrewAI](https://github.com/crewAIInc/crewAI) â­ 20k+ â€“ Multi-agent orchestration.
- ğŸŒ [AutoGen](https://github.com/microsoft/autogen) â­ 35k+ â€“ Microsoft's multi-agent framework.
- ğŸŒ [Semantic Kernel](https://github.com/microsoft/semantic-kernel) â€“ Microsoft's AI orchestration SDK.
- ğŸŒ [Haystack](https://github.com/deepset-ai/haystack) â€“ End-to-end NLP framework.
- ğŸŒ [PydanticAI](https://github.com/pydantic/pydantic-ai) â€“ Type-safe AI agent framework.

### Function Calling & Structured Output

- ğŸ§  [Mistral Function Calling](https://docs.mistral.ai/capabilities/function_calling/) â€“ Native function calling docs.
- ğŸŒ [Instructor](https://github.com/jxnl/instructor) â­ 8k+ â€“ Structured outputs with Pydantic.
- ğŸŒ [Outlines](https://github.com/outlines-dev/outlines) â­ 10k+ â€“ Guaranteed structured generation.
- ğŸŒ [Marvin](https://github.com/prefecthq/marvin) â€“ AI functions with type hints.

---

## Tooling & Dev Experience

### IDE Extensions & Code Assistants

- ğŸ§  [Zed Extensions](https://github.com/mistralai/zed-extensions) â€“ Official Mistral for Zed editor.
- ğŸŒ [Continue](https://github.com/continuedev/continue) â­ 20k+ â€“ Open-source AI code assistant (VSCode/JetBrains).
- ğŸŒ [Tabby](https://github.com/TabbyML/tabby) â­ 22k+ â€“ Self-hosted GitHub Copilot alternative.
- ğŸŒ [Aider](https://github.com/paul-gauthier/aider) â­ 20k+ â€“ AI pair programming in terminal.
- ğŸŒ [Cody](https://github.com/sourcegraph/cody) â€“ AI coding assistant with codebase context.

### Development Tools

- ğŸŒ [LiteLLM](https://github.com/BerriAI/litellm) â­ 15k+ â€“ Unified API for 100+ LLMs.
- ğŸŒ [Promptfoo](https://github.com/promptfoo/promptfoo) â­ 5k+ â€“ LLM evaluation and red-teaming.
- ğŸŒ [Langfuse](https://github.com/langfuse/langfuse) â­ 7k+ â€“ Open-source LLM observability.
- ğŸŒ [Phoenix](https://github.com/Arize-ai/phoenix) â€“ ML observability for LLM apps.
- ğŸŒ [Weights & Biases](https://wandb.ai) â€“ Experiment tracking with LLM support.

---

## Community Projects

### Chat Interfaces

- ğŸŒ [Open WebUI](https://github.com/open-webui/open-webui) â­ 50k+ â€“ Self-hosted ChatGPT-like UI.
- ğŸŒ [LibreChat](https://github.com/danny-avila/LibreChat) â­ 20k+ â€“ Multi-model chat interface.
- ğŸŒ [Lobe Chat](https://github.com/lobehub/lobe-chat) â­ 50k+ â€“ Modern extensible chat framework.
- ğŸŒ [Chatbot UI](https://github.com/mckaywrigley/chatbot-ui) â€“ Open-source ChatGPT clone.
- ğŸŒ [BetterChatGPT](https://github.com/ztjhz/BetterChatGPT) â€“ Enhanced chat interface.

### RAG & Knowledge Management

- ğŸŒ [PrivateGPT](https://github.com/zylon-ai/private-gpt) â­ 55k+ â€“ Private document Q&A.
- ğŸŒ [Danswer](https://github.com/danswer-ai/danswer) â­ 12k+ â€“ Enterprise Q&A over internal docs.
- ğŸŒ [Quivr](https://github.com/QuivrHQ/quivr) â­ 37k+ â€“ Personal knowledge base.
- ğŸŒ [Khoj](https://github.com/khoj-ai/khoj) â€“ AI second brain.
- ğŸŒ [LocalGPT](https://github.com/PromtEngineer/localGPT) â€“ Chat with documents locally.

### Specialized Applications

- ğŸŒ [Fabric](https://github.com/danielmiessler/fabric) â­ 25k+ â€“ AI augmentation framework.
- ğŸŒ [GPT Researcher](https://github.com/assafelovic/gpt-researcher) â­ 15k+ â€“ Autonomous research agent.
- ğŸŒ [OpenDevin](https://github.com/OpenDevin/OpenDevin) â­ 35k+ â€“ AI software engineer.

---

## Demos & Examples

### Official Examples

- ğŸ§  [Mistral Cookbook](https://github.com/mistralai/cookbook) â€“ RAG, function calling, embeddings, agents.
- ğŸ§  [Fine-Tuning Guide](https://docs.mistral.ai/capabilities/finetuning/) â€“ Official fine-tuning documentation.
- ğŸ§  [API Examples](https://docs.mistral.ai/api/) â€“ Complete API reference with examples.

### Community Examples

- ğŸŒ [Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM) â€“ Curated LLM resources including Mistral.
- ğŸŒ [LangChain Templates](https://github.com/langchain-ai/langchain/tree/master/templates) â€“ Production-ready templates.

---

## Tutorials & Guides

### Getting Started

- ğŸ§  [Mistral Quickstart](https://docs.mistral.ai/getting-started/quickstart/) â€“ Official getting started guide.
- ğŸ§  [Model Selection Guide](https://docs.mistral.ai/getting-started/models/) â€“ Choosing the right model.
- ğŸŒ [Run Mistral Locally](https://ollama.com/library/mistral) â€“ Ollama setup guide.

### Fine-Tuning Tutorials

- ğŸ§  [Official Fine-Tuning](https://docs.mistral.ai/capabilities/finetuning/) â€“ Mistral's fine-tuning guide.
- ğŸŒ [Axolotl Mistral Examples](https://github.com/axolotl-ai-cloud/axolotl/tree/main/examples/mistral) â€“ Config examples.
- ğŸŒ [QLoRA Guide](https://huggingface.co/blog/4bit-transformers-bitsandbytes) â€“ 4-bit fine-tuning.
- ğŸŒ [Unsloth Tutorial](https://github.com/unslothai/unsloth#mistral) â€“ Fast Mistral fine-tuning.

### RAG & Applications

- ğŸ§  [RAG with Mistral](https://docs.mistral.ai/guides/rag/) â€“ Official RAG guide.
- ğŸŒ [LlamaIndex + Mistral](https://docs.llamaindex.ai/en/stable/examples/llm/mistralai/) â€“ RAG with LlamaIndex.
- ğŸŒ [LangChain + Mistral](https://python.langchain.com/docs/integrations/llms/mistralai/) â€“ LangChain integration.

---

## Benchmarks & Evaluation

### Leaderboards

- ğŸŒ [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) â€“ Hugging Face benchmarks.
- ğŸŒ [Chatbot Arena](https://lmarena.ai/) â€“ Human preference rankings.
- ğŸŒ [Artificial Analysis](https://artificialanalysis.ai/) â€“ LLM quality and speed benchmarks.

### Evaluation Frameworks

- ğŸŒ [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) â€“ EleutherAI's eval framework.
- ğŸŒ [HELM](https://github.com/stanford-crfm/helm) â€“ Stanford's holistic evaluation.
- ğŸŒ [OpenCompass](https://github.com/open-compass/opencompass) â€“ Comprehensive LLM evaluation.

### Code Benchmarks

- ğŸŒ [HumanEval](https://github.com/openai/human-eval) â€“ Code generation benchmark.
- ğŸŒ [BigCodeBench](https://github.com/bigcode-project/bigcodebench) â€“ Comprehensive code evaluation.
- ğŸŒ [EvalPlus](https://github.com/evalplus/evalplus) â€“ Rigorous code evaluation.

---

## Research & Papers

### Mistral Technical Reports

- ğŸ§  [Mistral 7B](https://arxiv.org/abs/2310.06825) â€“ Foundational 7B architecture paper.
- ğŸ§  [Mixtral of Experts](https://arxiv.org/abs/2401.04088) â€“ Sparse MoE architecture.

### Related Research

- ğŸŒ [Sliding Window Attention](https://arxiv.org/abs/2004.05150) â€“ Longformer attention mechanism.
- ğŸŒ [LoRA](https://arxiv.org/abs/2106.09685) â€“ Low-Rank Adaptation paper.
- ğŸŒ [QLoRA](https://arxiv.org/abs/2305.14314) â€“ Quantized LoRA for efficient fine-tuning.
- ğŸŒ [DPO](https://arxiv.org/abs/2305.18290) â€“ Direct Preference Optimization.
- ğŸŒ [Mixture of Experts](https://arxiv.org/abs/1701.06538) â€“ MoE foundations.

---

## Talks & Media

### Official Channels

- ğŸ§  [Mistral AI Blog](https://mistral.ai/news/) â€“ Official announcements.
- ğŸ§  [Mistral AI Discord](https://discord.gg/mistralai) â€“ Official community server.
- ğŸ§  [Mistral AI Twitter/X](https://twitter.com/MistralAI) â€“ Official updates.

### Conferences & Talks

- ğŸŒ [Hugging Face YouTube](https://www.youtube.com/@HuggingFace) â€“ Tutorials with Mistral.
- ğŸŒ [AI Explained](https://www.youtube.com/@aiexplained-official) â€“ Technical breakdowns.

---

## Ecosystem & Community

### Cloud Providers

- ğŸŒ [Azure AI](https://azure.microsoft.com/en-us/products/ai-studio/) â€“ Mistral on Azure AI Studio.
- ğŸŒ [AWS Bedrock](https://aws.amazon.com/bedrock/) â€“ Mistral via Amazon Bedrock.
- ğŸŒ [Google Cloud Vertex AI](https://cloud.google.com/vertex-ai) â€“ Mistral on GCP.
- ğŸŒ [Groq](https://groq.com/) â€“ Ultra-fast Mistral inference.
- ğŸŒ [Together AI](https://together.ai/) â€“ Mistral model hosting.
- ğŸŒ [Replicate](https://replicate.com/) â€“ Run Mistral via API.

### Community Hubs

- ğŸŒ [Hugging Face Hub](https://huggingface.co/mistralai) â€“ Official model repository.
- ğŸ§  [Mistral Discord](https://discord.gg/mistralai) â€“ Official community.
- ğŸŒ [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/) â€“ Local LLM community.
- ğŸŒ [r/MistralAI](https://www.reddit.com/r/MistralAI/) â€“ Mistral-focused subreddit.

### Partnerships

- ğŸ§  [Microsoft Azure Partnership](https://azure.microsoft.com/en-us/blog/microsoft-and-mistral-ai-announce-new-partnership-to-accelerate-ai-innovation-and-introduce-mistral-large-first-on-azure/) â€“ Strategic Azure partnership.
- ğŸ§  [La Plateforme](https://console.mistral.ai/) â€“ Mistral's cloud platform.

---

## Contributing

Contributions are welcome! Please read the [contribution guidelines](CONTRIBUTING.md) before submitting a pull request.

### Quick Guidelines

1. Ensure all links point to real, existing resources
2. Use consistent formatting: `- ğŸ§ /ğŸŒ/ğŸ§ª [Name](url) â€“ Brief description.`
3. Prefer high-signal, actively maintained projects
4. Include star counts for major projects (â­ 10k+)

---

## License

[![CC0](https://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg)](https://creativecommons.org/publicdomain/zero/1.0/)

This work is licensed under [CC0 1.0 Universal](LICENSE).
